{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WNN_pytorch_regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPpnOK1R19AjzH7tgyTMTnf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanisasu/10k-sample/blob/main/WNN_pytorch_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWoflTRt0oKa"
      },
      "source": [
        "**Required imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjrxyQ587x7H"
      },
      "source": [
        "# General\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Scikit Learn\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "np.random.seed(2)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZLc8iFWVbbu"
      },
      "source": [
        "**Morlet Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUCPJInd721B"
      },
      "source": [
        "# Morlet Function\n",
        "def w_func(t):\n",
        "  return torch.cos(1.75*t)* torch.exp(-(t**2))"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGvyJhqVi4E"
      },
      "source": [
        "**Intialising Hyper parameters and constants**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj3Q1vGx75Lg"
      },
      "source": [
        "batch_size= 32\n",
        "lr = 0.001\n",
        "mom = 0.9\n",
        "epochs=100\n",
        "filename = '/content/Dataset_spine.csv'\n",
        "data = pd.read_csv(filename)\n",
        "nin = len(data.columns)-1 # Size of input layer (sample size, number of features) \n",
        "nhn = 10 # Size of hidden layer\n",
        "non = 1"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57Crl7fdVwbF"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqWcSAL7_yj"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = weight_norm(nn.Linear(nin, nhn, bias=False))\n",
        "        self.fc2 = weight_norm(nn.Linear(nhn, non, bias=False))\n",
        "        self.a = nn.Parameter(torch.rand(nhn), requires_grad=True)\n",
        "        self.b = nn.Parameter(torch.rand(nhn), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        t = (self.fc1(x)-self.b)/self.a\n",
        "        vk = self.fc2(w_func(t))\n",
        "        return vk\n",
        "\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CzaQOdTWCF7"
      },
      "source": [
        "**Preprocessing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVsJdGsHt-GW"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x,y):\n",
        "        self.data = []\n",
        "        for i,j in zip(x,y):\n",
        "            self.data.append((i,j))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    @classmethod \n",
        "    def splits(cls, filename):\n",
        "        \n",
        "        data = pd.read_csv(filename)\n",
        "        cols_name = data.loc[:].columns\n",
        "        nonumeric_cols = data.loc[:, data.dtypes == object].columns\n",
        "        \n",
        "        #replace empty values with NaN\n",
        "        data = data.replace(\"?\",\"NaN\")\n",
        "        \n",
        "        #convert non numeric columns to numeric values\n",
        "        for col_name in nonumeric_cols:\n",
        "            list_total = list(data[col_name])\n",
        "            list_uniq = list(set((list_total)))\n",
        "            data[col_name] = [list_uniq.index(data) for data in list_total]\n",
        "        \n",
        "        #replace null values with mean\n",
        "        for col_name in cols_name:\n",
        "          data[col_name].fillna(data[col_name].mean())\n",
        "\n",
        "        # Normalizing Data\n",
        "        data[data.columns] = minmax_scale(data[data.columns])\n",
        "        \n",
        "        #extract x and y values from data\n",
        "        data_pre = np.array(data)\n",
        "        x=data_pre[:,:-1]\n",
        "        y=data_pre[:,-1]\n",
        "\n",
        "        #split the data into train and test and convert to torch\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "        y_train=  torch.from_numpy(np.expand_dims(y_train, axis=1)).float()\n",
        "        y_test=  torch.from_numpy(np.expand_dims(y_test, axis=1)).float()\n",
        "        x_train =  torch.from_numpy(x_train).float()\n",
        "        x_test =  torch.from_numpy(x_test).float()\n",
        "        \n",
        "        train = cls(x_train, y_train)\n",
        "        test = cls(x_test, y_test)\n",
        "        return train, test"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2QDQK8FYP8P"
      },
      "source": [
        "**Average Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orntrKxB8HMo"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyH8TvBOYVWL"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grJZB5LJs0ji",
        "outputId": "c886cb1f-c295-4f72-96ee-0e43283bd835"
      },
      "source": [
        "#start time\n",
        "start_time = time.time()\n",
        "\n",
        "#intialiaze dataset, model, optimiser and losses\n",
        "model = Net()\n",
        "loss = 0\n",
        "losses = AverageMeter()\n",
        "total_loss = 0\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum=mom)\n",
        "train, test = CustomDataset.splits(filename)\n",
        "loss_array = []\n",
        "\n",
        "#dataloader\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "#train model\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    #check for exsisting checkpoint\n",
        "    if os.path.exists(\"best.pt\"):\n",
        "      checkpoint = torch.load(\"best.pt\")\n",
        "      best_loss = checkpoint['loss']\n",
        "    else:\n",
        "      best_loss = 9999\n",
        "\n",
        "    for x,y in train_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            x = x.to('cuda')\n",
        "            y = y.to(\"cuda\")\n",
        "            model.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #forward propogation and find losses (predict)\n",
        "        pred = model(x)\n",
        "        loss = nn.MSELoss()(pred, y)\n",
        "        total_loss += loss\n",
        "\n",
        "        #back propogation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.update(loss.item())\n",
        "    loss_array.append(losses.val)\n",
        "\n",
        "    #update checkpoint if loss is less than exsisting checkpoint\n",
        "    if losses.val < best_loss:\n",
        "      torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': losses.val,\n",
        "      }, \"best.pt\")\n",
        "      \n",
        "    print_str = 'Epoch: [{0}]\\tLoss= {loss.val:.6f} Loss Avg: ({loss.avg:.6f})\\t' .format(epoch, loss=losses) \n",
        "    print(print_str)\n",
        "\n",
        "#end time\n",
        "end_time = time.time()\n",
        "\n",
        "print('\\n')\n",
        "print(\"Time taken for the run\",end_time-start_time)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]\tLoss= 0.578545 Loss Avg: (0.727406)\t\n",
            "Epoch: [1]\tLoss= 0.500047 Loss Avg: (0.690307)\t\n",
            "Epoch: [2]\tLoss= 0.689166 Loss Avg: (0.665864)\t\n",
            "Epoch: [3]\tLoss= 0.552855 Loss Avg: (0.648098)\t\n",
            "Epoch: [4]\tLoss= 0.538248 Loss Avg: (0.634158)\t\n",
            "Epoch: [5]\tLoss= 0.508258 Loss Avg: (0.621415)\t\n",
            "Epoch: [6]\tLoss= 0.473061 Loss Avg: (0.608243)\t\n",
            "Epoch: [7]\tLoss= 0.440325 Loss Avg: (0.594089)\t\n",
            "Epoch: [8]\tLoss= 0.414626 Loss Avg: (0.578199)\t\n",
            "Epoch: [9]\tLoss= 0.331204 Loss Avg: (0.560364)\t\n",
            "Epoch: [10]\tLoss= 0.333444 Loss Avg: (0.541452)\t\n",
            "Epoch: [11]\tLoss= 0.277338 Loss Avg: (0.522082)\t\n",
            "Epoch: [12]\tLoss= 0.296836 Loss Avg: (0.503266)\t\n",
            "Epoch: [13]\tLoss= 0.240196 Loss Avg: (0.485770)\t\n",
            "Epoch: [14]\tLoss= 0.226850 Loss Avg: (0.469586)\t\n",
            "Epoch: [15]\tLoss= 0.269542 Loss Avg: (0.455096)\t\n",
            "Epoch: [16]\tLoss= 0.260897 Loss Avg: (0.442014)\t\n",
            "Epoch: [17]\tLoss= 0.251613 Loss Avg: (0.430205)\t\n",
            "Epoch: [18]\tLoss= 0.268411 Loss Avg: (0.419601)\t\n",
            "Epoch: [19]\tLoss= 0.259806 Loss Avg: (0.409982)\t\n",
            "Epoch: [20]\tLoss= 0.225538 Loss Avg: (0.401189)\t\n",
            "Epoch: [21]\tLoss= 0.240117 Loss Avg: (0.393198)\t\n",
            "Epoch: [22]\tLoss= 0.199232 Loss Avg: (0.385817)\t\n",
            "Epoch: [23]\tLoss= 0.243761 Loss Avg: (0.379093)\t\n",
            "Epoch: [24]\tLoss= 0.157492 Loss Avg: (0.372782)\t\n",
            "Epoch: [25]\tLoss= 0.182607 Loss Avg: (0.366971)\t\n",
            "Epoch: [26]\tLoss= 0.227895 Loss Avg: (0.361639)\t\n",
            "Epoch: [27]\tLoss= 0.214460 Loss Avg: (0.356649)\t\n",
            "Epoch: [28]\tLoss= 0.174523 Loss Avg: (0.351948)\t\n",
            "Epoch: [29]\tLoss= 0.230443 Loss Avg: (0.347606)\t\n",
            "Epoch: [30]\tLoss= 0.199159 Loss Avg: (0.343504)\t\n",
            "Epoch: [31]\tLoss= 0.217394 Loss Avg: (0.339663)\t\n",
            "Epoch: [32]\tLoss= 0.229087 Loss Avg: (0.336052)\t\n",
            "Epoch: [33]\tLoss= 0.227469 Loss Avg: (0.332640)\t\n",
            "Epoch: [34]\tLoss= 0.254822 Loss Avg: (0.329436)\t\n",
            "Epoch: [35]\tLoss= 0.207567 Loss Avg: (0.326361)\t\n",
            "Epoch: [36]\tLoss= 0.219380 Loss Avg: (0.323449)\t\n",
            "Epoch: [37]\tLoss= 0.203718 Loss Avg: (0.320666)\t\n",
            "Epoch: [38]\tLoss= 0.229036 Loss Avg: (0.318034)\t\n",
            "Epoch: [39]\tLoss= 0.170150 Loss Avg: (0.315479)\t\n",
            "Epoch: [40]\tLoss= 0.257033 Loss Avg: (0.313104)\t\n",
            "Epoch: [41]\tLoss= 0.218051 Loss Avg: (0.310803)\t\n",
            "Epoch: [42]\tLoss= 0.226649 Loss Avg: (0.308604)\t\n",
            "Epoch: [43]\tLoss= 0.191806 Loss Avg: (0.306471)\t\n",
            "Epoch: [44]\tLoss= 0.228639 Loss Avg: (0.304449)\t\n",
            "Epoch: [45]\tLoss= 0.198598 Loss Avg: (0.302484)\t\n",
            "Epoch: [46]\tLoss= 0.214083 Loss Avg: (0.300602)\t\n",
            "Epoch: [47]\tLoss= 0.191223 Loss Avg: (0.298775)\t\n",
            "Epoch: [48]\tLoss= 0.222930 Loss Avg: (0.297033)\t\n",
            "Epoch: [49]\tLoss= 0.243383 Loss Avg: (0.295363)\t\n",
            "Epoch: [50]\tLoss= 0.218893 Loss Avg: (0.293735)\t\n",
            "Epoch: [51]\tLoss= 0.208933 Loss Avg: (0.292157)\t\n",
            "Epoch: [52]\tLoss= 0.194438 Loss Avg: (0.290620)\t\n",
            "Epoch: [53]\tLoss= 0.197481 Loss Avg: (0.289129)\t\n",
            "Epoch: [54]\tLoss= 0.189500 Loss Avg: (0.287684)\t\n",
            "Epoch: [55]\tLoss= 0.222807 Loss Avg: (0.286296)\t\n",
            "Epoch: [56]\tLoss= 0.217998 Loss Avg: (0.284944)\t\n",
            "Epoch: [57]\tLoss= 0.190184 Loss Avg: (0.283614)\t\n",
            "Epoch: [58]\tLoss= 0.171856 Loss Avg: (0.282310)\t\n",
            "Epoch: [59]\tLoss= 0.203773 Loss Avg: (0.281057)\t\n",
            "Epoch: [60]\tLoss= 0.259801 Loss Avg: (0.279865)\t\n",
            "Epoch: [61]\tLoss= 0.186714 Loss Avg: (0.278665)\t\n",
            "Epoch: [62]\tLoss= 0.256723 Loss Avg: (0.277529)\t\n",
            "Epoch: [63]\tLoss= 0.219699 Loss Avg: (0.276402)\t\n",
            "Epoch: [64]\tLoss= 0.191803 Loss Avg: (0.275288)\t\n",
            "Epoch: [65]\tLoss= 0.260024 Loss Avg: (0.274229)\t\n",
            "Epoch: [66]\tLoss= 0.192338 Loss Avg: (0.273162)\t\n",
            "Epoch: [67]\tLoss= 0.223062 Loss Avg: (0.272132)\t\n",
            "Epoch: [68]\tLoss= 0.254699 Loss Avg: (0.271139)\t\n",
            "Epoch: [69]\tLoss= 0.183597 Loss Avg: (0.270131)\t\n",
            "Epoch: [70]\tLoss= 0.207651 Loss Avg: (0.269155)\t\n",
            "Epoch: [71]\tLoss= 0.216660 Loss Avg: (0.268201)\t\n",
            "Epoch: [72]\tLoss= 0.205289 Loss Avg: (0.267259)\t\n",
            "Epoch: [73]\tLoss= 0.154863 Loss Avg: (0.266313)\t\n",
            "Epoch: [74]\tLoss= 0.169705 Loss Avg: (0.265389)\t\n",
            "Epoch: [75]\tLoss= 0.152838 Loss Avg: (0.264476)\t\n",
            "Epoch: [76]\tLoss= 0.159726 Loss Avg: (0.263580)\t\n",
            "Epoch: [77]\tLoss= 0.187676 Loss Avg: (0.262712)\t\n",
            "Epoch: [78]\tLoss= 0.191364 Loss Avg: (0.261859)\t\n",
            "Epoch: [79]\tLoss= 0.148349 Loss Avg: (0.261001)\t\n",
            "Epoch: [80]\tLoss= 0.216322 Loss Avg: (0.260183)\t\n",
            "Epoch: [81]\tLoss= 0.223513 Loss Avg: (0.259379)\t\n",
            "Epoch: [82]\tLoss= 0.196823 Loss Avg: (0.258576)\t\n",
            "Epoch: [83]\tLoss= 0.192828 Loss Avg: (0.257784)\t\n",
            "Epoch: [84]\tLoss= 0.143162 Loss Avg: (0.256985)\t\n",
            "Epoch: [85]\tLoss= 0.186295 Loss Avg: (0.256212)\t\n",
            "Epoch: [86]\tLoss= 0.170215 Loss Avg: (0.255443)\t\n",
            "Epoch: [87]\tLoss= 0.184354 Loss Avg: (0.254691)\t\n",
            "Epoch: [88]\tLoss= 0.183639 Loss Avg: (0.253954)\t\n",
            "Epoch: [89]\tLoss= 0.229733 Loss Avg: (0.253237)\t\n",
            "Epoch: [90]\tLoss= 0.115188 Loss Avg: (0.252489)\t\n",
            "Epoch: [91]\tLoss= 0.174292 Loss Avg: (0.251771)\t\n",
            "Epoch: [92]\tLoss= 0.127652 Loss Avg: (0.251048)\t\n",
            "Epoch: [93]\tLoss= 0.210788 Loss Avg: (0.250360)\t\n",
            "Epoch: [94]\tLoss= 0.183741 Loss Avg: (0.249669)\t\n",
            "Epoch: [95]\tLoss= 0.152012 Loss Avg: (0.248980)\t\n",
            "Epoch: [96]\tLoss= 0.157773 Loss Avg: (0.248300)\t\n",
            "Epoch: [97]\tLoss= 0.204631 Loss Avg: (0.247640)\t\n",
            "Epoch: [98]\tLoss= 0.157727 Loss Avg: (0.246973)\t\n",
            "Epoch: [99]\tLoss= 0.164552 Loss Avg: (0.246319)\t\n",
            "\n",
            "\n",
            "Time taken for the run 16.261449098587036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W38VPQLYbEO3"
      },
      "source": [
        "**Evaluate model and predict values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKbpnaijtFae",
        "outputId": "76c382ac-0557-4f0c-dd27-016f050e75c7"
      },
      "source": [
        "#pick the best model from checkpoint\n",
        "if os.path.exists(\"best.pt\"):\n",
        "  checkpoint = torch.load(\"best.pt\")\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "#evaluate model\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "losses = AverageMeter()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for x,y in test_loader:\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.to('cuda')\n",
        "        y = y.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "\n",
        "      #predict values and find losses\n",
        "      pred = model(x)\n",
        "      test_loss += nn.MSELoss()(pred, y)\n",
        "      losses.update(loss.item())\n",
        "      y = y.cpu()\n",
        "      pred = pred.cpu()\n",
        "\n",
        "      #store predicted and true values into lists\n",
        "      for y_d in y:\n",
        "        y_true.append(y_d.item())\n",
        "      for pred_d in pred:\n",
        "        y_pred.append(pred_d.item())\n",
        "\n",
        "print_str = 'Loss: {loss.val:.6f} Loss Avg: ({loss.avg:.6f})\\t'.format(loss=losses) \n",
        "print(print_str)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.164552 Loss Avg: (0.164552)\t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPbI1AH7tZBf",
        "outputId": "045540bc-ee54-4562-f45e-6dd21f93e985"
      },
      "source": [
        "mape = mean_absolute_error(y_true, y_pred)*100\n",
        "print(mape)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37.0305938586112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AwLzwcszUBu"
      },
      "source": [
        "rm /content/best.pt"
      ],
      "execution_count": 133,
      "outputs": []
    }
  ]
}